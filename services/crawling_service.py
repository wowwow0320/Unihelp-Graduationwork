# /services/crawling_service.py

import os
import time
import pandas as pd
import logging
import re
import shutil
import tempfile
import asyncio
import json # ğŸ‘ˆ json ëª¨ë“ˆ ì¶”ê°€
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from typing import Tuple, List, Optional, Dict

from core.config import settings

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CrawlingService:
    def __init__(self):
        self.USER_ID = settings.CROWLING_ID
        self.USER_PW = settings.CROWLING_PW
        self.max_posts_to_scrape = 10

    # ğŸ‘‡ [ìˆ˜ì •] ë°˜í™˜ íƒ€ì… íŒíŠ¸ ë³€ê²½: List[Dict]ê°€ ê³µì§€ ë°ì´í„° + íŒŒì¼ ê²½ë¡œ í¬í•¨
    async def crawl_yongin_notices_with_files(self) -> Tuple[Optional[List[Dict]], str]:
        """
        í¬ë¡¤ë§ í›„, ê° ê³µì§€ì‚¬í•­ ë°ì´í„°ì™€ í•´ë‹¹ ì²¨ë¶€íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ê°€ í¬í•¨ëœ
        ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ì™€ ì„ì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        ë°˜í™˜ê°’: (notice_data_with_paths, temp_dir_path)
        """
        result = await asyncio.to_thread(self._run_crawl_logic_for_send)
        return result

    # ğŸ‘‡ [ìˆ˜ì •] í•¨ìˆ˜ ì´ë¦„ ë° ë¡œì§ ë³€ê²½
    def _run_crawl_logic_for_send(self) -> Tuple[Optional[List[Dict]], str]:
        """í¬ë¡¤ë§ì„ ìˆ˜í–‰í•˜ê³ , ê° ê³µì§€ ë”•ì…”ë„ˆë¦¬ì— ì²¨ë¶€íŒŒì¼ ì „ì²´ ê²½ë¡œë¥¼ í¬í•¨ì‹œì¼œ ë°˜í™˜í•©ë‹ˆë‹¤."""

        save_dir = tempfile.mkdtemp(prefix="yongin_crawl_")
        download_dir = os.path.join(save_dir, 'downloads')
        os.makedirs(download_dir)

        log_file_path = os.path.join(save_dir, 'crawler.log')
        file_handler = logging.FileHandler(log_file_path, encoding='utf-8')
        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
        logger.addHandler(file_handler)

        logger.info(f'ì„ì‹œ ë°ì´í„° ì €ì¥ í´ë”: {save_dir}')
        logger.info(f'ì²¨ë¶€íŒŒì¼ ì €ì¥ í´ë”: {download_dir}')

        options = webdriver.ChromeOptions()
        prefs = {
            "download.default_directory": download_dir,
            "download.prompt_for_download": False,
            "download.directory_upgrade": True,
            "plugins.always_open_pdf_externally": True
        }
        options.add_experimental_option('prefs', prefs)
        options.add_argument('User-Agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36')
        options.add_argument('--headless')
        options.add_argument('--disable-gpu')
        options.add_argument("--window-size=1920,1080")
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')

        driver = None
        try:
            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
            wait = WebDriverWait(driver, 10)
            logger.info('WebDriver ì´ˆê¸°í™” ì™„ë£Œ.')
        except Exception as e:
            logger.error(f"WebDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            shutil.rmtree(save_dir)
            raise Exception(f"WebDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

        # ğŸ‘‡ [ìˆ˜ì •] ë°ì´í„°ë¥¼ ë°”ë¡œ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
        crawled_data_with_paths: List[Dict] = []

        try:
            # --- ë¡œê·¸ì¸ ---
            login_url = 'https://total.yongin.ac.kr/login.do'
            driver.get(login_url)
            wait.until(EC.element_to_be_clickable((By.ID, 'userid'))).send_keys(self.USER_ID)
            driver.find_element(By.ID, 'pwd').send_keys(self.USER_PW)
            wait.until(EC.element_to_be_clickable((By.ID, 'btn_login'))).click()
            try:
                WebDriverWait(driver, 2).until(EC.alert_is_present())
                alert = driver.switch_to.alert; alert_text = alert.text; alert.accept()
                raise ValueError(f"ë¡œê·¸ì¸ ì‹¤íŒ¨: {alert_text}")
            except TimeoutException: logger.info("ë¡œê·¸ì¸ ì„±ê³µ í™•ì¸ ì¤‘...")

            # --- ê²Œì‹œíŒ ì´ë™ ---
            full_menu_xpath = "//a[contains(@class, 'btn_fullmenu')]"
            wait.until(EC.element_to_be_clickable((By.XPATH, full_menu_xpath))).click()
            notice_link_xpath = "//div[@id='full_menu']//h3[contains(@onclick, '/stdt/board/notice/list.do')]"
            wait.until(EC.element_to_be_clickable((By.XPATH, notice_link_xpath))).click()
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'tr.hand')))

            # --- ê²Œì‹œë¬¼ ìˆœíšŒ ë° ë°ì´í„° ì¶”ì¶œ ---
            posts_on_page = driver.find_elements(By.CSS_SELECTOR, 'tr.hand')
            num_to_scrape = min(len(posts_on_page), self.max_posts_to_scrape)

            for i in range(num_to_scrape):
                notice_info = {} # í˜„ì¬ ê²Œì‹œê¸€ ì •ë³´ ì €ì¥ìš©
                attachment_full_paths = [] # í˜„ì¬ ê²Œì‹œê¸€ ì²¨ë¶€íŒŒì¼ ê²½ë¡œ ì €ì¥ìš©

                all_posts = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'tr.hand')))
                post_to_click = all_posts[i]
                try:
                    driver.execute_script("arguments[0].click();", post_to_click)
                except Exception as e:
                    logger.error(f"ê²Œì‹œë¬¼ {i+1} í´ë¦­ ì‹¤íŒ¨: {e}. ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'bbs_title')))
                html = driver.page_source
                soup = BeautifulSoup(html, 'html.parser')

                # ë°ì´í„° ì¶”ì¶œ
                title = re.sub(r"^\s*\[.*?\]\s*", "", soup.find('th', 'bbs_title').get_text(strip=True))
                meta_tag = soup.find('td', 'bbs_date')
                writer = meta_tag.contents[0].strip().split(':')[1].strip()
                date = meta_tag.find('span', class_='mr100').get_text(strip=True).split(':')[1].strip()
                content = soup.find('td', 'bbs_content').get_text(strip=True)

                notice_info['ì œëª©'] = title
                notice_info['ì‘ì„±ì'] = writer
                notice_info['ì‘ì„±ì¼'] = date
                notice_info['ë‚´ìš©'] = content

                # ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ê²½ë¡œ ì €ì¥
                post_attachment_filenames = [] # í˜„ì¬ ê²Œì‹œê¸€ì˜ íŒŒì¼ëª…ë§Œ (JSON ì €ì¥ìš©)
                try:
                    file_elements = driver.find_elements(By.CSS_SELECTOR, "td.bbs_file a")
                    if file_elements:
                        for file_element in file_elements:
                            file_name = file_element.text.strip()
                            post_attachment_filenames.append(file_name) # íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
                            
                            # ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
                            driver.execute_script("arguments[0].click();", file_element)
                            logger.info(f" - ë‹¤ìš´ë¡œë“œ ì‹¤í–‰: {file_name}")
                            time.sleep(3) # ë‹¤ìš´ë¡œë“œ ëŒ€ê¸°

                            # ğŸ‘‡ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œ ìƒì„± ë° ì €ì¥
                            full_file_path = os.path.join(download_dir, file_name)
                            if os.path.exists(full_file_path): # ë‹¤ìš´ë¡œë“œê°€ ì‹¤ì œë¡œ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸
                                attachment_full_paths.append(full_file_path)
                            else:
                                logger.warning(f" - íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ë˜ëŠ” ì‹œê°„ ë¶€ì¡±: {file_name}")
                except Exception as e:
                    logger.error(f"ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

                notice_info['ì²¨ë¶€íŒŒì¼'] = post_attachment_filenames # JSONì—ëŠ” íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸ ì €ì¥
                notice_info['attachment_full_paths'] = attachment_full_paths # ë‚´ë¶€ ì²˜ë¦¬ìš© ì „ì²´ ê²½ë¡œ

                crawled_data_with_paths.append(notice_info) # ìµœì¢… ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

                driver.back()
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'tr.hand')))

        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì„ì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œëŠ” ë°˜í™˜í•´ì•¼ í•¨ (ì •ë¦¬ ëª©ì )
            return None, save_dir
        finally:
            if driver:
                driver.quit()
            logger.removeHandler(file_handler)
            file_handler.close()

        if crawled_data_with_paths:
            # ğŸ‘‡ [ìˆ˜ì •] JSON íŒŒì¼ ìƒì„± ë¡œì§ ì¶”ê°€ (Spring ì „ì†¡ ì‹œ í•„ìš”)
            json_data_for_spring = []
            for item in crawled_data_with_paths:
                 # Springìœ¼ë¡œ ë³´ë‚¼ ë°ì´í„°ì—ì„œëŠ” attachment_full_paths ì œì™¸
                item_copy = item.copy()
                del item_copy['attachment_full_paths']
                json_data_for_spring.append(item_copy)
            
            # ì„ì‹œ ë””ë ‰í† ë¦¬ì— ì „ì²´ ë°ì´í„°ë¥¼ ë‹´ì€ data.json ì €ì¥ (ì˜µì…˜)
            try:
                json_path = os.path.join(save_dir, 'crawled_data.json')
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(json_data_for_spring, f, ensure_ascii=False, indent=4)
                logger.info("í¬ë¡¤ë§ ë°ì´í„° ìš”ì•½ JSON ì €ì¥ ì™„ë£Œ.")
            except Exception as json_err:
                logger.error(f"í¬ë¡¤ë§ ë°ì´í„° ìš”ì•½ JSON ì €ì¥ ì‹¤íŒ¨: {json_err}")
                
            return crawled_data_with_paths, save_dir
        else:
            logger.info("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, save_dir

crawling_service = CrawlingService()