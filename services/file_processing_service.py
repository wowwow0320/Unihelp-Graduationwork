import os
import re
import io
import json
import asyncio
from pathlib import Path
from typing import Optional, Tuple, List, Dict

import pandas as pd
from bs4 import BeautifulSoup
from docx import Document
from docx.oxml.table import CT_Tbl
from docx.oxml.text.paragraph import CT_P
from docx.table import Table
from docx.text.paragraph import Paragraph
from dotenv import load_dotenv
from llama_cloud_services import LlamaParse
from pdf2docx import Converter
# import pdfplumber  # LlamaParseë¥¼ ì •ë‹µì§€ë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ ë” ì´ìƒ í•„ìš” ì—†ìŒ

# LlamaParseì— ì „ë‹¬í•  íŒŒì‹± ì§€ì‹œì–´ (ì „ì²´ ë‚´ìš©)
PARSING_INSTRUCTION = (
    "ë³¸ë¬¸ê³¼ í‘œë¥¼ êµ¬ë¶„í•´ ì£¼ì„¸ìš”.\n"
    "- í‘œì˜ ì²« í–‰ì´ ì—¬ëŸ¬ ì¤„ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤ë©´, ì´ë¥¼ í—¤ë”ë¡œ ê°„ì£¼í•˜ê³  ë³‘í•©í•´ í•˜ë‚˜ì˜ í—¤ë”ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.\n"
    "- í‘œëŠ” ì •í™•íˆ html í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.\n"
    "- í‘œì˜ í—¤ë”ëŠ” ë°˜ë“œì‹œ ê° ì—´ì— ë§ì¶° ë¶„ë¦¬í•´ì£¼ì„¸ìš”.\n"
    "- ì¤„ ë°”ê¿ˆ íƒœê·¸(<br/>)ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì—¬ëŸ¬ ì¡°ê±´ì´ ìˆëŠ” ì…€ì€ ìŠ¬ë˜ì‹œ(/) ë˜ëŠ” ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•´ì£¼ì„¸ìš”.\n"
    "- ë³‘í•©ëœ ì…€ì€ í•´ë‹¹ ì—´ì— ë§ì¶° ë°˜ë³µ ì‚½ì…í•´ì£¼ì„¸ìš”.\n"
    "- ì—´ ìˆ˜ê°€ ë¶ˆê· í˜•í•œ í–‰ì€ ë¬´ì¡°ê±´ í—¤ë” ì—´ ìˆ˜ì— ë§ì¶° ì •ë ¬í•´ì£¼ì„¸ìš”.\n"
    "- ìš”ì•½ì´ë‚˜ ì„¤ëª…ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n"
)

class FileProcessorService:
    def __init__(self, upload_dir: str = "uploads"):
        self.upload_dir = Path(upload_dir)
        self.upload_dir.mkdir(parents=True, exist_ok=True)
        
        load_dotenv()
        api_key = os.environ.get("LLAMA_CLOUD_API_KEY")
        if not api_key:
            raise ValueError("LLAMA_CLOUD_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # LlamaParse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì „ì²´ ì„¤ì •)
        self.llama_parser = LlamaParse(
            api_key=api_key,
            parse_mode="parse_page_with_agent",
            model="openai-gpt-4-1",
            high_res_ocr=True,
            adaptive_long_table=True,
            outlined_table_extraction=True,
            output_tables_as_HTML=True,
            markdown_table_multiline_header_separator="<br />",
            system_prompt_append=PARSING_INSTRUCTION,
            page_separator="\n\nâ€”\n\n",
        )
        print("âœ… LlamaParse í´ë¼ì´ì–¸íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # --- 1. ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ---

    async def process_full_pipeline(self, pdf_path: str) -> Tuple[str, str, str, str]:
        """
        [ìµœì¢… í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸ (v3: LlamaParse ì •ë‹µì§€)]
        1. [Async] LlamaParse: í…ìŠ¤íŠ¸(.md) ì¶”ì¶œ + í˜ì´ì§€ ë§µ(ì •ë‹µì§€) ìƒì„±
        2. [Executor] pdf2docx: .docx íŒŒì¼ ìƒì„±
        3. [Executor] docx_parser + Matcher: .docxì™€ LlamaParse ë§µì„ ë§¤ì¹­í•´ .html, .txt ìƒì„±
        """
        print(f"ğŸš€ ì „ì²´ í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {pdf_path}")
        pdf_path_obj = Path(pdf_path)
        loop = asyncio.get_event_loop()

        # 1. LlamaParseë¡œ í…ìŠ¤íŠ¸(.md)ì™€ í˜ì´ì§€ ë§µ ì¶”ì¶œ (Async)
        print("ğŸ”§ [1/3] LlamaParseë¡œ í…ìŠ¤íŠ¸(.md) ë° í˜ì´ì§€ ë§µ ìƒì„± ì¤‘...")
        llama_task = self._parse_text_and_create_page_map_with_llama(pdf_path_obj)
        
        # 2. DOCX ë³€í™˜ (Sync í•¨ìˆ˜ë¥¼ Asyncë¡œ ì‹¤í–‰)
        print("ğŸ”§ [2/3] DOCX ë³€í™˜ ì‹œì‘...")
        docx_path_task = loop.run_in_executor(
            None, self.convert_pdf_to_docx, pdf_path
        )
        
        # ë³‘ë ¬ ì‘ì—… 1, 2ê°€ ì™„ë£Œë˜ê¸°ë¥¼ ê¸°ë‹¤ë¦¼
        (markdown_path, page_map), docx_path = await asyncio.gather(
            llama_task,
            docx_path_task
        )
        
        print(f"âœ… [1/3] Markdown (í…ìŠ¤íŠ¸) ë° í˜ì´ì§€ ë§µ ìƒì„± ì™„ë£Œ. ì´ {len(page_map)} í˜ì´ì§€.")
        print(f"âœ… [2/3] DOCX ì €ì¥ ì™„ë£Œ: {docx_path}")

        # 3. DOCX íŒŒì‹± ë° LlamaParse ë§µê³¼ í˜ì´ì§€ ë²ˆí˜¸ ë§¤ì¹­ (Sync í•¨ìˆ˜ë¥¼ Asyncë¡œ ì‹¤í–‰)
        print("ğŸ”§ [3/3] DOCX íŒŒì‹± ë° í˜ì´ì§€ ë²ˆí˜¸ ë§¤ì¹­ ì‹œì‘...")
        if not page_map:
             print("âš ï¸ í˜ì´ì§€ ë§µì´ ë¹„ì–´ìˆì–´ ë§¤ì¹­ì„ ê±´ë„ˆëœë‹ˆë‹¤. í˜ì´ì§€ ë²ˆí˜¸ê°€ -1ë¡œ í‘œì‹œë©ë‹ˆë‹¤.")
             
        html_path, rag_text_path = await loop.run_in_executor(
            None, 
            self._extract_tables_with_docx_and_matching,
            docx_path,
            page_map,
            pdf_path_obj.name # ë©”íƒ€ë°ì´í„°ìš©
        )
        
        print(f"âœ… [3/3] HTML (í…Œì´ë¸”) ë° RAG-TXT (K-V) ì €ì¥ ì™„ë£Œ: {html_path}, {rag_text_path}")
        print(f"ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ.")
        
        return docx_path, markdown_path, html_path, rag_text_path

    # --- 2. íŒŒì´í”„ë¼ì¸ êµ¬ì„± ìš”ì†Œ ---

    def convert_pdf_to_docx(self, pdf_path: str, start_page: int = 0, end_page: Optional[int] = None) -> str:
        """ [Task 2] PDFë¥¼ DOCXë¡œ ë³€í™˜ (ê¸°ì¡´ê³¼ ë™ì¼) """
        print(f"ğŸ“„ DOCX ë³€í™˜ ì‹œì‘: {pdf_path}")
        pdf_path_obj = Path(pdf_path)
        docx_path = pdf_path_obj.with_suffix(".docx")
        try:
            cv = Converter(str(pdf_path_obj))
            cv.convert(str(docx_path), start=start_page, end=end_page)
            cv.close()
            print(f"âœ… DOCX ì €ì¥ ì™„ë£Œ: {docx_path}")
        except Exception as e:
            print(f"âŒ DOCX ë³€í™˜ ì‹¤íŒ¨: {e}")
        return str(docx_path)

    async def _parse_text_and_create_page_map_with_llama(self, pdf_path_obj: Path) -> Tuple[str, Dict[int, str]]:
        """
        [Task 1] LlamaParseë¥¼ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸(.md)ì™€ í˜ì´ì§€ ë§µ(ì •ë‹µì§€)ì„ ë™ì‹œì— ìƒì„±í•©ë‹ˆë‹¤.
        """
        output_md_path = pdf_path_obj.with_suffix(".md")
        page_map: Dict[int, str] = {}
        processed_pages: List[str] = []
        
        try:
            result = await self.llama_parser.aparse(str(pdf_path_obj))
            markdown_documents = result.get_markdown_documents(split_by_page=True)
            
            for doc in markdown_documents:
                page_num = doc.metadata.get("page_number", -1)
                page_content_with_tables = doc.text # í…Œì´ë¸”ì´ í¬í•¨ëœ ì›ë³¸ í…ìŠ¤íŠ¸
                
                # 1. í˜ì´ì§€ ë§µ(ì •ë‹µì§€) ìƒì„±
                if page_num != -1:
                    # LlamaParseê°€ íŒŒì‹±í•œ (HTML í…Œì´ë¸” í¬í•¨) í…ìŠ¤íŠ¸ë¥¼ ì •ë‹µì§€ë¡œ ì‚¬ìš©
                    page_map[page_num] = page_content_with_tables 
                
                # 2. .md íŒŒì¼ìš© í…ìŠ¤íŠ¸ ìƒì„± (í…Œì´ë¸” ì œê±°)
                text_only = self._preprocess_text(page_content_with_tables)
                # processed_pages.append(f"\n{text_only}") # ì›ë³¸ ì½”ë“œ
                processed_pages.append(f"\n{text_only}") # ì‚¬ìš©ìë‹˜ì´ ìˆ˜ì •í•œ ì½”ë“œ

            # .md íŒŒì¼ ì“°ê¸°
            final_markdown = "\n\nâ€”\n\n".join(processed_pages)
            with open(output_md_path, "w", encoding="utf-8") as f_md:
                f_md.write(final_markdown)
                
        except Exception as e:
            print(f"âŒ LlamaParse ì‹¤íŒ¨: {e}")
            
        return str(output_md_path), page_map

    def _extract_tables_with_docx_and_matching(self, docx_path: str, page_map: Dict[int, str], pdf_name: str) -> Tuple[str, str]:
        """
        [Task 3] 'python-docx'ë¡œ í…Œì´ë¸”ì„ íŒŒì‹±í•˜ê³  'LlamaParse í˜ì´ì§€ ë§µ'ê³¼ ë§¤ì¹­í•˜ì—¬
        í˜ì´ì§€ ë²ˆí˜¸ê°€ í¬í•¨ëœ .htmlê³¼ .txtë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        html_path = Path(docx_path).with_suffix(".html")
        rag_text_path = Path(docx_path).with_suffix(".txt")
        
        all_kv_lines = []
        all_html_tables = []
        
        try:
            doc = Document(docx_path)
            
            # 1. DOCXì—ì„œ ëª¨ë“  <p>ì™€ <table>ì„ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ë™ì¼)
            full_parts = []
            section_blocks_list = self._get_section_blocks(doc)
            for idx, (section, section_blocks) in enumerate(zip(doc.sections, section_blocks_list), 1):
                body_parts = self._process_body(section_blocks)
                full_parts.extend(body_parts)
            
            full_html = self._prettify_html("\n".join(full_parts))
            soup = BeautifulSoup(full_html, "html.parser")

            # 2. HTMLì„ ìˆœíšŒí•˜ë©° í…Œì´ë¸”ê³¼ ì œëª© ì¶”ì¶œ, K-V ìƒì„±, í˜ì´ì§€ ë§¤ì¹­
            for table in soup.find_all("table"):
                table_html = str(table)
                
                # --- A. ì œëª© ì¶”ì¶œ ---
                title = "ì œëª© ì—†ìŒ"
                prev_p = table.find_previous("p")
                if prev_p and not self._is_footer_text(prev_p.get_text(strip=True)):
                    title = prev_p.get_text(strip=True)

                # --- B. K-V ë°ì´í„° ìƒì„± (pandas í™œìš©) ---
                try:
                    df_list = pd.read_html(io.StringIO(table_html), header=0)
                    if not df_list: continue
                    
                    df = df_list[0]
                    
                    # (ì‚¬ìš©ìë‹˜ì˜ ë‹¤ì¤‘ í—¤ë” ë¡œì§)
                    if not df.empty and df.iloc[0].astype(str).str.contains('ëŒ€ í•™').any():
                        new_header = df.iloc[0]
                        df = df[1:] # ì‹¤ì œ ë°ì´í„°ë§Œ ë‚¨ê¹€
                        df.columns = [f"{str(col).split('.')[0]} {val}" if 'Unnamed' not in str(col) else val for col, val in new_header.items()]
                    
                    if df.empty: continue

                    # --- C. í˜ì´ì§€ ë²ˆí˜¸ ë§¤ì¹­ (ë”ìš± ê²¬ê³ í•˜ê²Œ) ---
                    anchor_text_1 = "" # í…Œì´ë¸”ì˜ ì²« í–‰, ë‘ ë²ˆì§¸ ê°’ (ê³ ìœ í•  ê°€ëŠ¥ì„± ë†’ìŒ)
                    anchor_text_2 = "" # í…Œì´ë¸”ì˜ ì²« í–‰, ë§ˆì§€ë§‰ ê°’ (ì¶”ê°€ ê²€ì¦ìš©)
                    try:
                        anchor_text_1 = str(df.iloc[0, 1]).strip() # ì˜ˆ: "ê²½ì˜í•™ê³¼(ì£¼)"
                        anchor_text_2 = str(df.iloc[0, -1]).strip() # ì˜ˆ: "130"
                    except IndexError:
                        pass # í…Œì´ë¸”ì´ ë¹„ì—ˆê±°ë‚˜ êµ¬ì¡°ê°€ ì´ìƒí•˜ë©´ ì•µì»¤ ì—†ìŒ

                    found_page = -1
                    if (anchor_text_1 or anchor_text_2) and page_map:
                         found_page = self._find_page_for_anchor(
                             page_map, title, anchor_text_1, anchor_text_2
                         )
                    
                    # all_html_tables.append(f"\n# {title}\n") # ì›ë³¸ ì½”ë“œ
                    all_html_tables.append(f"\n# {title}\n") # ì‚¬ìš©ìë‹˜ì´ ìˆ˜ì •í•œ ì½”ë“œ
                    all_html_tables.append(table_html) # ì›ë³¸ HTML(êµ¬ì¡°ê°€ ì˜¬ë°”ë¥¸) ì €ì¥

                    # --- D. K-V ë¬¸ì¥ ìƒì„± ---
                    for _, row in df.iterrows():
                        row_data = ", ".join([
                            f"{col}: {val}" 
                            for col, val in row.items() 
                            if pd.notna(val) and str(val).strip()
                        ])
                        if not row_data: continue

                        sentence = f"ì œëª©: {title}, {row_data}"
                        
                        metadata = {
                            "source": pdf_name,
                            "page": found_page,
                            "type": "table_kv"
                        }
                        meta_json = json.dumps(metadata, ensure_ascii=False)
                        all_kv_lines.append(f"{meta_json} {sentence}")

                except Exception as e:
                    print(f"âš ï¸ í…Œì´ë¸” K-V ë³€í™˜/ë§¤ì¹­ ì˜¤ë¥˜ (ê±´ë„ˆëœë‹ˆë‹¤): {e}")

            # RAG-TXT íŒŒì¼ (K-V + ë©”íƒ€ë°ì´í„°) ì €ì¥
            with open(rag_text_path, 'w', encoding='utf-8') as f_txt:
                f_txt.write('\n'.join(all_kv_lines))
                
            # HTML íŒŒì¼ (í…Œì´ë¸” ì‹œê°í™”ìš©) ì €ì¥
            with open(html_path, 'w', encoding='utf-8') as f_html:
                f_html.write('\n\n'.join(all_html_tables))

        except Exception as e:
            print(f"âŒ DOCX íŒŒì‹± ë° ë§¤ì¹­ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
            
        return str(html_path), str(rag_text_path)

    def _normalize_text_for_matching(self, text: str) -> str:
        """ë§¤ì¹­ì„ ìœ„í•´ ê³µë°±, ì¤„ë°”ê¿ˆ ë“±ì„ ì •ê·œí™”í•©ë‹ˆë‹¤."""
        if not text:
            return ""
        # ëª¨ë“  ê³µë°± ë¬¸ì(ìŠ¤í˜ì´ìŠ¤, íƒ­, ì¤„ë°”ê¿ˆ)ë¥¼ ë‹¨ì¼ ìŠ¤í˜ì´ìŠ¤ë¡œ ë³€í™˜
        return re.sub(r'\s+', ' ', text).strip()

    def _find_page_for_anchor(self, page_map: dict, title: str, anchor_1: str, anchor_2: str) -> int:
        """
        LlamaParse í˜ì´ì§€ ë§µì—ì„œ ì œëª©ê³¼ 2ê°œì˜ ì•µì»¤ í…ìŠ¤íŠ¸ë¡œ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        ê³µë°±/ì¤„ë°”ê¿ˆ ì°¨ì´ë¥¼ ë¬´ì‹œí•˜ê¸° ìœ„í•´ ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        
        # ë§¤ì¹­ì„ ìœ„í•´ ì•µì»¤ í…ìŠ¤íŠ¸ì™€ ì œëª©ì„ ì •ê·œí™”í•©ë‹ˆë‹¤.
        norm_title = self._normalize_text_for_matching(title)
        norm_anchor_1 = self._normalize_text_for_matching(anchor_1)
        norm_anchor_2 = self._normalize_text_for_matching(anchor_2)

        # 1ìˆœìœ„: ì•µì»¤ 2ê°œê°€ ëª¨ë‘ ì¼ì¹˜ (ê°€ì¥ ì‹ ë¢°ë„ ë†’ìŒ)
        if norm_anchor_1 and norm_anchor_2:
            for page_num, page_text in page_map.items():
                norm_page_text = self._normalize_text_for_matching(page_text)
                if norm_anchor_1 in norm_page_text and norm_anchor_2 in norm_page_text:
                    return page_num
        
        # 2ìˆœìœ„: ì•µì»¤ 1ë§Œ ì¼ì¹˜
        if norm_anchor_1:
            for page_num, page_text in page_map.items():
                norm_page_text = self._normalize_text_for_matching(page_text)
                if norm_anchor_1 in norm_page_text:
                    return page_num
                    
        # 3ìˆœìœ„: ì œëª©ê³¼ ì•µì»¤ 1ì´ ì¼ì¹˜
        if norm_title != "ì œëª© ì—†ìŒ" and norm_anchor_1:
            for page_num, page_text in page_map.items():
                norm_page_text = self._normalize_text_for_matching(page_text)
                if norm_title in norm_page_text and norm_anchor_1 in norm_page_text:
                    return page_num
        
        return -1 # ëª¨ë‘ ì‹¤íŒ¨

    # --- 3. ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ (LlamaParseìš©) ---

    def _preprocess_text(self, text: str) -> str:
        """ LlamaParse ê²°ê³¼ë¬¼ì—ì„œ HTML í…Œì´ë¸”ì„ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤. """
        text_without_tables = re.sub(r"<table.*?</table>", "", text, flags=re.DOTALL | re.IGNORECASE)
        text = text_without_tables.replace('\r\n', '\n')
        text = re.sub(r'<br\s*/?>', ' / ', text)
        text = re.sub(r'(?<!\n)\n(?!\n)', ' ', text) # ë‹¨ì¼ ì¤„ë°”ê¿ˆì€ ê³µë°±ìœ¼ë¡œ
        text = re.sub(r'\n{2,}', '\n\n', text) # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì€ ë‹¨ë½ìœ¼ë¡œ
        text = re.sub(r' +', ' ', text)
        return text.strip()

    # --- 4. ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ (python-docx íŒŒì‹±ìš© - ì›ë³¸ ë³µì›) ---

    def _prettify_html(self, html: str) -> str:
        html = re.sub(r'>\s+<', '><', html)
        html = re.sub(r'(</[^>]+>)', r'\1\n', html)
        return html.strip()

    def _iter_block_items(self, element, doc):
        """ docxì˜ body ìš”ì†Œë¥¼ ìˆœíšŒí•˜ëŠ” ì œë„ˆë ˆì´í„° """
        for child in element.iterchildren():
            if isinstance(child, CT_P): yield Paragraph(child, doc)
            elif isinstance(child, CT_Tbl): yield Table(child, doc)

    def _extract_textbox_texts(self, paragraph):
        """ ë‹¨ë½ ë‚´ í…ìŠ¤íŠ¸ ë°•ìŠ¤ í…ìŠ¤íŠ¸ ì¶”ì¶œ """
        ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}
        texts = []
        for txbx in paragraph._element.findall('.//w:txbxContent', ns):
            for t in txbx.findall('.//w:t', ns):
                if t.text: texts.append(t.text)
        return texts

    def _get_section_blocks(self, doc):
        """ docxë¥¼ êµ¬ì—­(section)ë³„ ë¸”ë¡ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬ """
        body = doc.element.body
        blocks = list(self._iter_block_items(body, doc))
        section_boundaries, current_blocks = [], []
        for blk in blocks:
            current_blocks.append(blk)
            if isinstance(blk, Paragraph) and blk._element.find('.//w:sectPr', {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}) is not None:
                section_boundaries.append(current_blocks)
                current_blocks = []
        if current_blocks: section_boundaries.append(current_blocks)
        return section_boundaries

    def _is_footer_text(self, text: str) -> bool:
        """ ë¨¸ë¦¬ê¸€/ë°”ë‹¥ê¸€ì˜ í˜ì´ì§€ ë²ˆí˜¸ ë“±ì„ í•„í„°ë§í•˜ê¸° ìœ„í•œ í•¨ìˆ˜ """
        text = text.strip()
        if not text: return True
        if re.fullmatch(r"-?\s*\d{1,4}\s*-?", text): return True
        if re.fullmatch(r"(p\.?|page)\s*\d{1.4}", text, re.IGNORECASE): return True
        return False

    def _process_header_footer(self, section_part, label):
        """ (ì°¸ê³ ) ë¨¸ë¦¬ê¸€/ë°”ë‹¥ê¸€ ì²˜ë¦¬ í•¨ìˆ˜ (í˜„ì¬ ë©”ì¸ ë¡œì§ì—ì„  ì‚¬ìš© ì•ˆ í•¨) """
        parts = []
        for para in section_part.paragraphs:
            combined_text = para.text.strip()
            for tb_text in self._extract_textbox_texts(para):
                if tb_text not in combined_text: combined_text += tb_text
            if combined_text and not self._is_footer_text(combined_text):
                parts.append(f"<p class='header'>[{label}] {combined_text}</p>")
        for table in section_part.tables:
            html_table = "<table border='1' class='header'>\n"
            for row in table.rows:
                html_table += "<tr>" + "".join(f"<td>{'<br>'.join(p.text.strip() for p in cell.paragraphs if p.text.strip())}</td>" for cell in row.cells) + "</tr>\n"
            html_table += "</table>\n"
            parts.append(html_table)
        return parts

    def _process_body(self, blocks):
        """ docx ë³¸ë¬¸ ë¸”ë¡(ë‹¨ë½, í…Œì´ë¸”)ì„ HTMLë¡œ ë³€í™˜ """
        parts = []
        for block in blocks:
            if isinstance(block, Paragraph):
                text = block.text.strip()
                if text and not self._is_footer_text(text): parts.append(f"<p>{text}</p>")
                tb_combined = "".join(self._extract_textbox_texts(block)).strip()
                if tb_combined and not self._is_footer_text(tb_combined): parts.append(f"<p>{tb_combined}</p>")
            elif isinstance(block, Table):
                html_table = "<table border='1'>\n"
                for row in block.rows:
                    html_table += "<tr>" + "".join(f"<td>{'<br>'.join(p.text.strip() for p in cell.paragraphs if p.text.strip())}</td>" for cell in row.cells) + "</tr>\n"
                html_table += "</table>\n"
                parts.append(html_table)
        return parts