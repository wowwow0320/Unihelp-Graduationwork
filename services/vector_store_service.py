# services/vector_store_service.py

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_core.documents import Document
from core.config import settings
from models.llm_factory import embedding_model
from typing import List, Optional, Dict, Any  # ğŸ‘ˆ [ìˆ˜ì •]

class VectorStoreService:
    def __init__(self):
        self.db_path = settings.DB_PATH
        self.embedding_model = embedding_model

    def _load_db(self, collection_name: str) -> Chroma:
        if not collection_name:
            raise ValueError("Collection name must be provided.")
        return Chroma(
            persist_directory=self.db_path,
            embedding_function=self.embedding_model,
            collection_name=collection_name
        )

    def _process_markdown_file(self, file_path: str) -> list[Document]:
        loader = TextLoader(file_path, encoding="utf-8")
        docs = loader.load()
        headers_to_split_on = [("#", "Header 1")]
        markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
        md_header_splits = markdown_splitter.split_text(docs[0].page_content)
        char_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        return char_splitter.split_documents(md_header_splits)


    def _process_table_text_file(self, file_path: str) -> list[Document]:
        """
        'Key: Value' í˜•íƒœì˜ í…ìŠ¤íŠ¸ íŒŒì¼ì„ í•œ ì¤„ì”© ì½ì–´,
        - ì›ë³¸ í…ìŠ¤íŠ¸ ì „ì²´ëŠ” page_contentì— ì €ì¥í•˜ê³ ,
        - íŒŒì‹±ëœ Key:Value ìŒì€ metadataì— ë™ì ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        """
        documents = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                # 1. ì›ë³¸ í•œ ì¤„ì„ ê·¸ëŒ€ë¡œ page_contentë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
                #    ì´ë ‡ê²Œ í•˜ë©´ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì‹œ ëª¨ë“  ì •ë³´ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                page_content = line.strip()
                if not page_content:
                    continue

                try:
                    # 2. ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬ë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
                    metadata = {}
                    parts = page_content.split(', ')
                    for part in parts:
                        if ': ' in part:
                            key, value = part.split(': ', 1)
                            metadata[key.strip()] = value.strip()
                    
                    # 3. page_contentì™€ ë™ì ìœ¼ë¡œ ìƒì„±ëœ metadataë¡œ Document ê°ì²´ë¥¼ ë§Œë“­ë‹ˆë‹¤.
                    documents.append(Document(page_content=page_content, metadata=metadata))
                
                except Exception as e:
                    print(f"âš ï¸ í•œ ì¤„ì„ ë©”íƒ€ë°ì´í„°ë¡œ íŒŒì‹±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ (ê±´ë„ˆëœë‹ˆë‹¤): {page_content}")
                    print(f"   ì˜¤ë¥˜ ë‚´ìš©: {e}")

        return documents

    def build_from_files(self, md_path: str, txt_path: str, collection_name: str):
        db = self._load_db(collection_name)
        md_chunks = self._process_markdown_file(md_path)
        txt_chunks = self._process_table_text_file(txt_path)
        all_chunks = md_chunks + txt_chunks

        if not all_chunks:
            print("ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        batch_size = 64
        for i in range(0, len(all_chunks), batch_size):
            batch = all_chunks[i:i + batch_size]
            db.add_documents(batch)
            print(f"-> {min(i + batch_size, len(all_chunks))}/{len(all_chunks)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ...")
        
        print(f"\nğŸ‰ ì»¬ë ‰ì…˜ '{collection_name}'ì˜ Chroma DB ì—…ë°ì´íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # DBì— ì €ì¥ëœ ëª¨ë“  ì»¬ë ‰ì…˜ ëª©ë¡ì„ ë°˜í™˜í•˜ëŠ” ë©”ì„œë“œ
    def list_collections(self) -> List[str]:
        """
        Chroma DBì— ì €ì¥ëœ ëª¨ë“  ì»¬ë ‰ì…˜ì˜ ì´ë¦„ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        # íŠ¹ì • ì»¬ë ‰ì…˜ ì´ë¦„ ì—†ì´ Chroma í´ë¼ì´ì–¸íŠ¸ì— ì—°ê²°
        client = Chroma(
            persist_directory=self.db_path,
            embedding_function=self.embedding_model
        )
        
        collections = client._client.list_collections()
        
        # ì»¬ë ‰ì…˜ ê°ì²´ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì´ë¦„ë§Œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜
        return [col.name for col in collections] if collections else []

    # def get_retriever(self, collection_name: str):
    #     db = self._load_db(collection_name)
    #     return db.as_retriever(
    #         search_type="mmr",
    #         search_kwargs={"k": 10, "lambda_mult": 0.9, "fetch_k": 20}
    #     )
    # â¬‡ï¸ [ìˆ˜ì •]
    # â¬‡ï¸ [ìˆ˜ì •] metadata_filterì™€ document_filterë¥¼ ëª¨ë‘ ë°›ë„ë¡ ë³€ê²½
    def get_retriever(
        self, 
        collection_name: str, 
        metadata_filter: Optional[Dict[str, Any]] = None,
        document_filter: Optional[Dict[str, Any]] = None  # ğŸ‘ˆ [ì¶”ê°€]
    ):
        db = self._load_db(collection_name)
        
        search_kwargs = {
            "k": 20
        }
        search_type = "similarity" 
            
        # 1. [ì¶”ê°€] ë©”íƒ€ë°ì´í„° í•„í„° (where)
        if metadata_filter:
            search_kwargs["filter"] = metadata_filter
        
        # 2. [ì¶”ê°€] ë¬¸ì„œ ë³¸ë¬¸ í•„í„° (where_document)
        if document_filter:
            search_kwargs["where_document"] = document_filter # ğŸ‘ˆ [ì¶”ê°€]

        return db.as_retriever(
            search_type=search_type,
            search_kwargs=search_kwargs
        )
vector_store_service = VectorStoreService()